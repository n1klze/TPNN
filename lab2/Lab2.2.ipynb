{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mushroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mlp import MLP, load_weights, save_weights\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(filepath: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    data = pd.read_csv(\"mushrooms.csv\", delimiter=\",\")\n",
    "    X = data.drop(\"class\", axis=1)\n",
    "    y = data[\"class\"].apply(lambda x: np.float64(1) if x == \"p\" else np.float64(0))\n",
    "    X = pd.get_dummies(X, dtype=np.float64).fillna(0)\n",
    "    X = X.drop(\"stalk-root_?\", axis=1).values\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:06<00:00,  3.05it/s]\n"
     ]
    }
   ],
   "source": [
    "X, y = load_and_preprocess_data(\"mushrooms.csv\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "input_size = X_train.shape[1]\n",
    "mlp = MLP(input_size=input_size, hidden_sizes=[(input_size + 1) // 2], output_size=1)\n",
    "# mlp = load_weights(\"mushrooms_weights.pkl\")\n",
    "\n",
    "mlp.train(X_train, y_train, learning_rate=1e-3, epochs=20)\n",
    "save_weights(mlp, \"mushrooms_weights.pkl\")\n",
    "\n",
    "predictions = mlp.predict(X_test) > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "$$Accuracy = \\frac{TP + TN}{TP + FN + TN + FP}$$\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "$$Recall = \\frac{TP}{TP + FN}$$\n",
    "$$F1—score = \\frac{2 \\cdot precision \\cdot recall}{precision + recall}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9870769230769231\n",
      "Precision: 1.0\n",
      "Recall: 0.9726918075422627\n",
      "F1-score: 0.9861568885959131\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(y_true: list[float], y_pred: list[float]) -> Dict[str, float]:\n",
    "    tp = sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) else 0\n",
    "    f1_score = (\n",
    "        2 * (precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "    )\n",
    "    accuracy = (tp + tn) / (tp + fn + tn + fp)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-score\": f1_score,\n",
    "    }\n",
    "\n",
    "\n",
    "metrics = calculate_metrics(y_test, predictions.flatten())\n",
    "\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
